[返回目录](../MLY_index.html)

# 22. 比较最佳错误率 Comparing to the optimal error rate

2018-10-10

[TOC]

## 学习收获

> sh

内容



In our cat recognition example, the “ideal” error rate—that is, one achievable by an “optimal” classifier—is nearly 0%. A human looking at a picture would be able to recognize if it contains a cat almost all the time; thus, we can hope for a machine that would do just as well.

在我们的猫识别示例中，“理想”错误率 - 即“最佳”分类器可实现的错误率 - 几乎为0％。看着图片的人几乎可以一直识别它是否包含一只猫;因此，我们可以希望一台能做得同样的机器。

Other problems are harder. For example, suppose that you are building a speech recognition system, and find that 14% of the audio clips have so much background noise or are so unintelligible that even a human cannot recognize what was said. In this case, even the most “optimal” speech recognition system might have error around 14%.

其他问题更难。例如，假设您正在构建一个语音识别系统，并发现14％的音频片段具有如此多的背景噪音或者是如此难以理解，即使是人类也无法识别所说的内容。在这种情况下，即使是最“最佳”的语音识别系统也可能有大约14％的误差。

Suppose that on this speech recognition problem, your algorithm achieves:

假设在这个语音识别问题上，您的算法实现了：

- Training error = 15%
- Dev error = 30% 

The training set performance is already close to the optimal error rate of 14%. Thus, there is not much room for improvement in terms of bias or in terms of training set performance.

训练集的性能已经接近14％的最佳错误率。因此，在偏见或训练集性能方面没有太大的改进空间。

However, this algorithm is not generalizing well to the dev set; thus there is ample room for improvement in the errors due to variance.

但是，该算法并没有很好地推广到开发集;因此，由于变化导致的误差存在很大的改进空间。

This example is similar to the third example from the previous chapter, which also had a training error of 15% and dev error of 30%. If the optimal error rate is ~0%, then a training error of 15% leaves much room for improvement. This suggests bias-reducing changes might be fruitful. But if the optimal error rate is 14%, then the same training set performance tells us that there’s little room for improvement in the classifier’s bias.

这个例子类似于前一章的第三个例子，它的训练误差为15％，dev误差为30％。如果最佳错误率为~0％，那么15％的训练误差留下了很大的改进空间。这表明减少偏见的变化可能是富有成效的。但如果最佳错误率为14％，则相同的训练集性能告诉我们，分类器的偏差几乎没有改进的余地。

For problems where the optimal error rate is far from zero, here’s a more detailed breakdown of an algorithm’s error. Continuing with our speech recognition example above, the total dev set error of 30% can be broken down as follows (a similar analysis can be applied to the test set error):

对于最佳错误率远为零的问题，这里是算法错误的更详细分类。继续上面的语音识别示例，30％的总设置错误可以分解如下（类似的分析可以应用于测试集错误）：

- **Optimal error rate (“unavoidable bias”)**: 14%. Suppose we decide that, even with the best possible speech system in the world, we would still suffer 14% error. We can think of this as the “unavoidable” part of a learning algorithm’s bias.

  最佳错误率（“不可避免的偏见”）：14％。假设我们决定，即使世界上最好的语音系统，我们仍然会遭受14％的错误。我们可以将此视为学习算法偏差的“不可避免”部分。

- **Avoidable bias**: 1%. This is calculated as the difference between the training error and the optimal error rate.^8^

  可避免的偏见：1％。这是以训练误差和最佳误差率之间的差值计算的

- **Variance**: 15%. The difference between the dev error and the training error.

  差异：15％。 dev错误和训练错误之间的区别。

To relate this to our earlier definitions, Bias and Avoidable Bias are related as follows:^9^ 

为了将其与我们之前的定义联系起来，偏差和可避免偏差的关系如下：9

​		Bias = Optimal error rate (“unavoidable bias”) + Avoidable bias

The “avoidable bias” reflects how much worse your algorithm performs on the training set than the “optimal classifier.”

“可避免的偏差”反映了算法在训练集上的表现比“最佳分类器”差多少。

The concept of variance remains the same as before. In theory, we can always reduce variance to nearly zero by training on a massive training set. Thus, all variance is “avoidable” with a sufficiently large dataset, so there is no such thing as “unavoidable variance.”

方差的概念与以前一样。从理论上讲，我们总是可以通过训练大量训练集将方差减少到接近零。因此，对于足够大的数据集，所有方差都是“可避免的”，因此不存在“不可避免的方差”。

Consider one more example, where the optimal error rate is 14%, and we have:

再考虑一个例子，最佳错误率为14％，我们有：

- Training error = 15%
- Dev error = 16%

Whereas in the previous chapter we called this a high bias classifier, now we would say that error from avoidable bias is 1%, and the error from variance is about 1%. Thus, the algorithm is already doing well, with little room for improvement. It is only 2% worse than the optimal error rate.

在前一章中我们将其称为高偏差分类器，现在我们可以说可避免偏差的误差为1％，方差误差约为1％。因此，该算法已经做得很好，几乎没有改进的余地。它比最佳错误率差2％。

We see from these examples that knowing the optimal error rate is helpful for guiding our next steps. In statistics, the optimal error rate is also called **Bayes error rate**, or Bayes rate.

我们从这些例子中看到，了解最佳错误率有助于指导我们的后续步骤。在统计中，最佳错误率也称为贝叶斯错误率或贝叶斯率。

How do we know what the optimal error rate is? For tasks that humans are reasonably good at, such as recognizing pictures or transcribing audio clips, you can ask a human to provide labels then measure the accuracy of the human labels relative to your training set. This would give an estimate of the optimal error rate. If you are working on a problem that even humans have a hard time solving (e.g., predicting what movie to recommend, or what ad to show to a user) it can be hard to estimate the optimal error rate.

我们如何知道最佳错误率是多少？对于人类相当擅长的任务，例如识别图片或抄录音频片段，您可以要求人类提供标签，然后测量人体标签相对于训练集的准确性。这将给出最佳错误率的估计。如果您正在解决甚至人类难以解决的问题（例如，预测要推荐的电影或向用户展示的广告），则很难估计最佳错误率。

In the section “Comparing to Human-Level Performance (Chapters 33 to 35), I will discuss in more detail the process of comparing a learning algorithm’s performance to human-level performance.

在“与人类绩效比较（第33至35章）”一节中，我将更详细地讨论将学习算法的性能与人类绩效进行比较的过程。

In the last few chapters, you learned how to estimate avoidable/unavoidable bias and variance by looking at training and dev set error rates. The next chapter will discuss how you can use insights from such an analysis to prioritize techniques that reduce bias vs. techniques that reduce variance. There are very different techniques that you should apply depending on whether your project’s current problem is high (avoidable) bias or high variance. Read on!

在最后几章中，您学习了如何通过查看训练和开发设置错误率来估计可避免/不可避免的偏差和方差。下一章将讨论如何使用此类分析中的见解来优先考虑减少偏差的技术与减少差异的技术。根据项目当前问题是高（可避免）偏差还是高差异，您应该应用非常不同的技术。继续阅读！



-----

8 If this number is negative, you are doing better on the training set than the optimal error rate. This means you are overfitting on the training set, and the algorithm has over-memorized the training set. You should focus on variance reduction methods rather than on further bias reduction methods.

8 如果此数字为负数，则您在训练集上的表现优于最佳错误率。这意味着您在训练集上过度拟合，并且该算法已经过度记忆训练集。您应该关注方差减少方法，而不是进一步减少偏差方法。

9 These definitions are chosen to convey insight on how to improve your learning algorithm. These definitions are different than how statisticians define Bias and Variance. Technically, what I define here as “Bias” should be called “Error we attribute to bias”; and “Avoidable bias” should be “error we attribute to the learning algorithm’s bias that is over the optimal error rate.”

9 选择这些定义是为了传达有关如何改进学习算法的见解。这些定义与统计学家定义偏差和方差的方式不同。从技术上讲，我在这里定义的“偏见”应该被称为“错误我们归因于偏见”;并且“可避免的偏差”应该是“我们认为学习算法偏差超过最佳错误率的错误”。