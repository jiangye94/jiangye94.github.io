[返回目录](../MLY_index.html)

# 24. 偏差与差异权衡 Bias vs. Variance tradeoff 

2018-10-10

[TOC]

## 学习收获

> sh

内容



You might have heard of the “Bias vs. Variance tradeoff.” Of the changes you could make to most learning algorithms, there are some that reduce bias errors but at the cost of increasing variance, and vice versa. This creates a “trade off” between bias and variance. 

你可能听说过“偏差与方差权衡”。你可以对大多数学习算法做出的改变，有一些可以减少偏差，但代价是增加方差，反之亦然。这在偏差和方差之间产生了“折衷”。

For example, increasing the size of your model—adding neurons/layers in a neural network, or adding input features—generally reduces bias but could increase variance. Alternatively, adding regularization generally increases bias but reduces variance. 

例如，在神经网络中增加模型添加神经元/层的大小，或添加输入特征 - 通常会减少偏差，但可能会增加差异。或者，添加正则化通常会增加偏差但会减少差异。

In the modern era, we often have access to plentiful data and can use very large neural networks (deep learning). Therefore, there is less of a tradeoff, and there are now more options for reducing bias without hurting variance, and vice versa. 

在现代，我们经常可以访问大量数据，并且可以使用非常大的神经网络（深度学习）。因此，有较少的权衡，现在有更多的选择来减少偏差而不会损害方差，反之亦然。

For example, you can usually increase a neural network size and tune the regularization method to reduce bias without noticeably increasing variance. By adding training data, you can also usually reduce variance without affecting bias. 

例如，您通常可以增加神经网络大小并调整正则化方法以减少偏差而不会显着增加方差。通过添加训练数据，您通常还可以在不影响偏差的情况下减少方差。

If you select a model architecture that is well suited for your task, you might also reduce bias and variance simultaneously. Selecting such an architecture can be difficult. 

如果选择适合您任务的模型体系结构，则还可以同时减少偏差和方差。选择这样的架构可能很困难。

In the next few chapters, we discuss additional specific techniques for addressing bias and variance. 

在接下来的几章中，我们将讨论用于解决偏差和方差的其他特定技术。