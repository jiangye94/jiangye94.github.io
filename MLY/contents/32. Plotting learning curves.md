[返回目录](../MLY_index.html)

# 32. Plotting learning curves

2018-10-11

[TOC]

## 学习收获

> sh

内容

Suppose you have a very small training set of 100 examples. You train your algorithm using a randomly chosen subset of 10 examples, then 20 examples, then 30, up to 100, increasing the number of examples by intervals of ten. You then use these 10 data points to plot your learning curve. You might find that the curve looks slightly noisy (meaning that the values are higher/lower than expected) at the smaller training set sizes.

假设您有一个非常小的训练集，包含100个示例。您使用随机选择的10个示例的子集训练算法，然后是20个示例，然后是30个，最多100个，以10为间隔增加示例数。然后，您可以使用这10个数据点来绘制学习曲线。在较小的训练集大小中，您可能会发现曲线看起来有点嘈杂（意味着值高于/低于预期值）。

When training on just 10 randomly chosen examples, you might be unlucky and have a particularly “bad” training set, such as one with many ambiguous/mislabeled examples. Or, you might get lucky and get a particularly “good” training set. Having a small training set means that the dev and training errors may randomly fluctuate.

在仅对10个随机选择的示例进行训练时，您可能运气不佳并且具有特别“差”的训练集，例如具有许多模糊/错误标记的示例的训练集。或者，你可能会很幸运，并得到一个特别“好”的训练集。具有小的训练集意味着开发和训练错误可能随机波动。

If your machine learning application is heavily skewed toward one class (such as a cat classification task where the fraction of negative examples is much larger than positive examples), or if it has a huge number of classes (such as recognizing 100 different animal species), then the chance of selecting an especially “unrepresentative” or bad training set is also larger. For example, if 80% of your examples are negative examples (y=0), and only 20% are positive examples (y=1), then there is a chance that a training set of 10 examples contains only negative examples, thus making it very difficult for the algorithm to learn something meaningful.

如果您的机器学习应用程序严重偏向于一个类（例如猫分类任务，其中负例的比例远远大于正例），或者它有大量的类（例如识别100种不同的动物种类）那么选择特别“不具代表性”或不良训练集的机会也更大。例如，如果80％的示例是负面示例（y = 0），并且只有20％是正面示例（y = 1），那么10个示例的训练集可能只包含负面示例，从而使算法很难学到有意义的东西。

If the noise in the training curve makes it hard to see the true trends, here are two solutions:

如果训练曲线中的噪声使得很难看到真实的趋势，这里有两个解决方案：

- Instead of training just one model on 10 examples, instead select several (say 3-10) different randomly chosen training sets of 10 examples by sampling with replacement^10^ from your original set of 100. Train a different model on each of these, and compute the training and dev set error of each of the resulting models. Compute and plot the average training error and average dev set error.

  而不是仅仅训练10个示例中的一个模型，而是选择几个（比如3-10个）不同的随机选择的10个示例的训练集，通过从原始集合100中的替换10进行采样。在每个示例上训练不同的模型，并计算每个结果模型的训练和开发设置错误。计算并绘制平均训练误差和平均开发设置误差。

- If your training set is skewed towards one class, or if it has many classes, choose a “balanced” subset instead of 10 training examples at random out of the set of 100. For example, you can make sure that 2/10 of the examples are positive examples, and 8/10 are negative. More generally, you can make sure the fraction of examples from each class is as close as possible to the overall fraction in the original training set.

  如果你的训练集偏向一个类，或者它有很多类，那么在100个集合中随机选择一个“平衡”子集而不是10个训练样例。例如，你可以确保2/10的这些例子是积极的例子，8/10是否定的。更一般地说，您可以确保每个类中的示例部分尽可能接近原始训练集中的总分数。

I would not bother with either of these techniques unless you have already tried plotting learning curves and concluded that the curves are too noisy to see the underlying trends. If your training set is large—say over 10,000 examples—and your class distribution is not very skewed, you probably won’t need these techniques.

除非你已经尝试绘制学习曲线并得出曲线太嘈杂以至于看不到潜在的趋势，否则我不会理会这些技术。如果您的训练集很大 - 比如说超过10,000个例子 - 并且您的班级分布不是很严重，那么您可能不需要这些技巧。

Finally, plotting a learning curve may be computationally expensive: For example, you might have to train ten models with 1,000, then 2,000, all the way up to 10,000 examples. Training models with small datasets is much faster than training models with large datasets. Thus, instead of evenly spacing out the training set sizes on a linear scale as above, you might train models with 1,000, 2,000, 4,000, 6,000, and 10,000 examples. This should still give you a clear sense of the trends in the learning curves. Of course, this technique is relevant only if the computational cost of training all the additional models is significant.

最后，绘制学习曲线可能在计算上很昂贵：例如，您可能需要训练10个模型，其中1,000个，然后是2,000个，一直到10,000个示例。使用小型数据集的训练模型比具有大型数据集的训练模型快得多。因此，您可以训练具有1,000,2,000,4,000,6,000和10,000个示例的模型，而不是如上所述在线性刻度上均匀地间隔训练集大小。这仍然可以让您清楚地了解学习曲线的趋势。当然，只有在训练所有附加模型的计算成本很高的情况下，该技术才是相关的。



----

10 Here’s what sampling *with replacement* means: You would randomly pick 10 different examples out of the 100 to form your first training set. Then to form the second training set, you would again pick 10 examples, but without taking into account what had been chosen in the first training set. Thus, it is possible for one specific example to appear in both the first and second training sets. In contrast, if you were sampling *without replacement*, the second training set would be chosen from just the 90 examples that had not been chosen the first time around. In practice, sampling with or without replacement shouldn’t make a huge difference, but the former is common practice.

10以下是使用替换的抽样方法：您将从100中随机选择10个不同的示例来形成您的第一个训练集。然后，为了形成第二个训练集，你将再次挑选10个例子，但不考虑在第一个训练集中选择的内容。因此，一个特定示例可以出现在第一和第二训练集中。相比之下，如果您在没有替换的情况下进行采样，那么第二次训练集将仅从第一次未选择的90个示例中选择。在实践中，有或没有替换的采样不应该产生巨大的差异，但前者是常见的做法。