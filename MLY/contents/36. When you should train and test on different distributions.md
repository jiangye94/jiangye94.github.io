[返回目录](../MLY_index.html)

# 36. 什么时候应该训练和测试不同的分布 When you should train and test on different distributions

2018-10-13

[TOC]

## 学习收获

> sh

内容

Users of your cat pictures app have uploaded 10,000 images, which you have manually labeled as containing cats or not. You also have a larger set of 200,000 images that you downloaded off the internet. How should you define train/dev/test sets?

您的猫图片应用程序的用户上传了10,000张图片，您手动将其标记为包含猫或不包含猫。您还可以从互联网上下载更多的200,000张图像。你应该如何定义火车/开发/测试集？

Since the 10,000 user images closely reflect the actual probability distribution of data you want to do well on, you might use that for your dev and test sets. If you are training a data-hungry deep learning algorithm, you might give it the additional 200,000 internet images for training. Thus, your training and dev/test sets come from different probability distributions. How does this affect your work?

由于10,000个用户图像紧密反映了您想要做的数据的实际概率分布，因此您可以将它用于开发和测试集。如果您正在训练需要数据的深度学习算法，您可以为其提供额外的200,000个互联网图像以进行培训。因此，您的训练和开发/测试集来自不同的概率分布。这对您的工作有何影响？

Instead of partitioning our data into train/dev/test sets, we could take all 210,000 images we have, and randomly shuffle them into train/dev/test sets. In this case, all the data comes from the same distribution. But I recommend against this method, because about 205,000/210,000 ≈ 97.6% of your dev/test data would come from internet images, which does not reflect the actual distribution you want to do well on. Remember our recommendation on choosing dev/test sets:

我们可以拍摄所有210,000张图像，然后随机将它们随机混合到训练/开发/测试集中，而不是将我们的数据划分为训练/开发/测试集。在这种情况下，所有数据都来自同一分布。但我建议不要使用这种方法，因为大约205,000 /210,000≈97.6％的开发/测试数据来自互联网图像，这并不能反映您想要做的实际分布。记住我们关于选择开发/测试集的建议：

> Choose dev and test sets to reflect data you expect to get in the future and want to do well on.
>
> 选择开发和测试集以反映您希望将来获得的数据，并希望做得好。

Most of the academic literature on machine learning assumes that the training set, dev set and test set all come from the same distribution.^11^ In the early days of machine learning, data was scarce. We usually only had one dataset drawn from some probability distribution. So we would randomly split that data into train/dev/test sets, and the assumption that all the data was coming from the same source was usually satisfied.

大多数关于机器学习的学术文献假设训练集，开发集和测试集都来自同一分布.11在机器学习的早期，数据很少。我们通常只有一个数据集来自某个概率分布。因此，我们会将这些数据随机分成训练/开发/测试集，并且通常会满足所有数据来自同一来源的假设。

But in the era of big data, we now have access to huge training sets, such as cat internet images. Even if the training set comes from a different distribution than the dev/test set, we still want to use it for learning since it can provide a lot of information.

但是在大数据时代，我们现在可以访问大量的培训集，例如猫互联网图像。即使训练集来自与开发/测试集不同的发行版，我们仍然希望将其用于学习，因为它可以提供大量信息。

For the cat detector example, instead of putting all 10,000 user-uploaded images into the dev/test sets, we might instead put 5,000 into the dev/test sets. We can put the remaining 5,000 user-uploaded examples into the training set. This way, your training set of 205,000 examples contains some data that comes from your dev/test distribution along with the 200,000 internet images. We will discuss in a later chapter why this method is helpful.

对于cat探测器示例，我们可能会将5,000个放入开发/测试集中，而不是将所有10,000个用户上载的图像放入开发/测试集中。我们可以将剩余的5,000个用户上传的示例放入训练集中。这样，您的205,000个示例的训练集包含来自您的开发/测试分发的一些数据以及200,000个互联网图像。我们将在后面的章节中讨论为什么这种方法很有用。

Let’s consider a second example. Suppose you are building a speech recognition system to transcribe street addresses for a voice-controlled mobile map/navigation app. You have 20,000 examples of users speaking street addresses. But you also have 500,000 examples of other audio clips with users speaking about other topics. You might take 10,000 examples of street addresses for the dev/test sets, and use the remaining 10,000, plus the additional 500,000 examples, for training.

让我们考虑第二个例子。假设您正在构建语音识别系统来转录语音控制的移动地图/导航应用的街道地址。您有20,000个用户说街道地址的示例。但是，您还有500,000个其他音频剪辑示例，其中包含有关其他主题的用户。您可以为开发/测试集获取10,000个街道地址示例，并使用剩余的10,000个以及额外的500,000个示例进行培训。

We will continue to assume that your dev data and your test data come from the same distribution. But it is important to understand that different training and dev/test distributions offer some special challenges.

我们将继续假设您的开发数据和测试数据来自同一分布。但重要的是要了解不同的培训和开发/测试分发提供了一些特殊的挑战。



---

11 There is some academic research on training and testing on different distributions. Examples include “domain adaptation,” “transfer learning” and “multitask learning.” But there is still a huge gap between theory and practice. If you train on dataset A and test on some very different type of data B, luck could have a huge effect on how well your algorithm performs. (Here, “luck” includes the researcher’s hand-designed features for the particular task, as well as other factors that we just don’t understand yet.) This makes the academic study of training and testing on different distributions difficult to carry out in a systematic way.

11有一些关于不同分布的培训和测试的学术研究。例子包括“领域适应”，“转移学习”和“多任务学习”。但理论与实践之间仍存在巨大差距。如果您在数据集A上进行训练并测试一些非常不同类型的数据B，那么运气可能会对算法的执行效果产生巨大影响。 （这里，“运气”包括研究人员针对特定任务的手工设计特征，以及我们尚未理解的其他因素。）这使得对不同分布的训练和测试的学术研究难以实现。一种系统的方式。