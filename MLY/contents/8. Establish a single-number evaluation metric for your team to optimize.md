[返回目录](../MLY_index.html)

# 8. 建立单值评估指标 Establish a single-number evaluation metric for your team to optimize

2018-10-08

[TOC]

## 学习收获

> 用单值指标评估模型
>
> 准确率
>
> 精度、召回率 -> 计算平均值、F1 Score
>
> 多个指标合并成单值指标



Classification accuracy is an example of a **single-number evaluation metric**: You run your classifier on the dev set (or test set), and get back a single number about what fraction of examples it classified correctly. According to this metric, if classifier A obtains 97% accuracy, and classifier B obtains 90% accuracy, then we judge classifier A to be superior.

分类准确率是单值评估度量的一个例子：您在开发集（或测试集）上运行分类器，并返回一个单一的数值，表示它正确分类的示例的比例。根据该度量，如果分类器A获得97％的准确率，并且分类器B获得90％的准确率，则我们判断分类器A更好。

In contrast, Precision and Recall^3^ is not a single-number evaluation metric: It gives two numbers for assessing your classifier. Having multiple-number evaluation metrics makes it harder to compare algorithms. Suppose your algorithms perform as follows:

相比之下，Precision和Recall^3^不是单值评估指标：它为评估您的分类器提供了两个指标。拥有多个数值评估指标会使算法的比较变得更加困难。假设您的算法表现如下：

| **Classifier** | **Precision** | **Recall** |
| -------------- | ------------- | ---------- |
| **A**          | 95%           | 90%        |
| **B**          | 98%           | 85%        |

Here, neither classifier is obviously superior, so it doesn’t immediately guide you toward picking one.

在这里，两个分类器都没有明显的优势，所以它无法立即指导您选择哪一个。

| **Classifier** | **Precision** | **Recall** | **F1 score** |
| -------------- | ------------- | ---------- | ------------ |
| **A**          | 95%           | 90%        | **92.4%**    |

During development, your team will try a lot of ideas about algorithm architecture, model parameters, choice of features, etc. Having a **single-number evaluation metric** such as accuracy allows you to sort all your models according to their performance on this metric, and quickly decide what is working best.

在开发过程中，您的团队将尝试大量想法，如算法架构、模型参数，特征选择等等。拥有单值评估指标（如准确性）允许您根据其在此指标上的表现对所有模型进行排序，以及快速决定什么是最好的。

If you really care about both Precision and Recall, I recommend using one of the standard ways to combine them into a single number. For example, one could take the average of precision and recall, to end up with a single number. Alternatively, you can compute the “F1 score,” which is a modified way of computing their average, and works better than simply taking the mean.^4^ 

如果你真的关心Precision和Recall，我建议使用一种标准方法将它们组合成一个数字。例如，可以采用精确度和召回率的平均值，最终得到一个数字。或者，您可以计算“F1得分”，这是一种改进后的计算平均值的方式，并且比简单地采用均值更有效.^4^

| **Classifier** | **Precision** | **Recall** | **F1 score** |
| -------------- | ------------- | ---------- | ------------ |
| **A**          | 95%           | 90%        | **92.4%**    |
| **B**          | 98%           | 85%        | **91.0%**    |

Having a single-number evaluation metric speeds up your ability to make a decision when you are selecting among a large number of classifiers. It gives a clear preference ranking among all of them, and therefore a clear direction for progress.

当您在大量分类器中进行选择时，使用单值评估指标使您更快地做出决策。它可以对您选择的所有分类器给出明确的偏好排名，因此可以明确进展的方向。

As a final example, suppose you are separately tracking the accuracy of your cat classifier in four key markets: (i) US, (ii) China, (iii) India, and (iv) Other. This gives four metrics. By taking an average or weighted average of these four numbers, you end up with a single number metric. Taking an average or weighted average is one of the most common ways to combine multiple metrics into one.

作为最后一个例子，假设您在四个主要市场中分别跟踪猫分类器的准确性：（i）美国，（ii）中国，（iii）印度，以及（iv）其他。这样就得到四个指标。通过计算这四个数字的平均值或加权平均值，您最终得到一个数值指标。采用平均或加权平均值是将多个指标合并为一个的最常见方法之一。



------------

3 The Precision of a cat classifier is the fraction of images in the dev (or test) set it labeled as cats that really are cats. Its Recall is the percentage of all cat images in the dev (or test) set that it correctly labeled as a cat. There is often a tradeoff between having high precision and high recall.

3 猫分类器的Precision是开发（或测试）中图像的一小部分，它标记为猫的图片中真正是猫的比例。Recall是开发（或测试）集中正确标记为猫与所有猫图像的百分比。在高精度和高召回率之间经常存在权衡。

4 If you want to learn more about the F1 score, see <https://en.wikipedia.org/wiki/F1_score>. It is the “harmonic mean” between Precision and Recall, and is calculated as 2/((1/Precision)+(1/Recall)).

4 如果您想了解有关F1分数的更多信息，请参阅https://en.wikipedia.org/wiki/F1_score。它是Precision和Recall之间的“调和平均值”，计算公式为2 /（（1 / Precision）+（1 / Recall））。