[返回目录](../MLY_index.html)

# 27. 减少差异的技术 Techniques for reducing variance

2018-10-10

[TOC]

## 学习收获

> sh

内容



If your learning algorithm suffers from high variance, you might try the following techniques:

如果您的学习算法存在高差异，您可以尝试以下技术：

- **Add more training data**: This is the simplest and most reliable way to address variance, so long as you have access to significantly more data and enough computational power to process the data.

  添加更多训练数据：这是解决方差的最简单，最可靠的方法，只要您可以访问更多的数据和足够的计算能力来处理数据。

- **Add regularization** (L2 regularization, L1 regularization, dropout): This technique reduces variance but increases bias.

  添加正则化（L2正则化，L1正则化，丢失）：该技术减少了方差但增加了偏差。

- **Add early stopping** (i.e., stop gradient descent early, based on dev set error): This technique reduces variance but increases bias. Early stopping behaves a lot like regularization methods, and some authors call it a regularization technique.

  添加早期停止（即，基于开发设置误差，提前停止梯度下降）：此技术可减少方差但增加偏差。早期停止的行为与正则化方法非常相似，有些作者称之为正则化技术。

- **Feature selection to decrease number/type of input features:** This technique might help with variance problems, but it might also increase bias. Reducing the number of features slightly (say going from 1,000 features to 900) is unlikely to have a huge effect on bias. Reducing it significantly (say going from 1,000 features to 100—a 10x reduction) is more likely to have a significant effect, so long as you are not excluding too many useful features. In modern deep learning, when data is plentiful, there has been a shift away from feature selection, and we are now more likely to give all the features we have to the algorithm and let the algorithm sort out which ones to use based on the data. But when your training set is small, feature selection can be very useful.

  用于减少输入要素数量/类型的特征选择：此技术可能有助于解决方差问题，但也可能会增加偏差。稍微减少功能的数量（比如从1000个功能到900个）不太可能对偏差产生巨大影响。只要你没有排除太多有用的功能，减少它（例如从1000个功能减少到100个减少10倍）就更有可能产生重大影响。在现代深度学习中，当数据充足时，已经从特征选择中转移，现在我们更有可能将所有特征赋予算法，并让算法根据数据选择使用哪些特征。 。但是当您的训练集很小时，功能选择可能非常有用。

- **Decrease the model size** (such as number of neurons/layers): *Use with caution.* This technique could decrease variance, while possibly increasing bias. However, I don’t recommend this technique for addressing variance. Adding regularization usually gives better classification performance. The advantage of reducing the model size is reducing your computational cost and thus speeding up how quickly you can train models. If speeding up model training is useful, then by all means consider decreasing the model size. But if your goal is to reduce variance, and you are not concerned about the computational cost, consider adding regularization instead.

  减小模型大小（例如神经元/层数）：谨慎使用。该技术可以减少方差，同时可能增加偏差。但是，我不推荐这种技术来解决方差。添加正则化通常会提供更好的分类性能。减小模型尺寸的优势在于降低了计算成本，从而加快了训练模型的速度。如果加速模型训练很有用，那么一定要考虑减小模型大小。但是，如果您的目标是减少方差，并且您不关心计算成本，请考虑添加正则化。

Here are two additional tactics, repeated from the previous chapter on addressing bias:

以下是关于解决偏见的前一章重复的另外两种策略：

- **Modify input features based on insights from error analysis**: Say your error analysis inspires you to create additional features that help the algorithm to eliminate a particular category of errors. These new features could help with both bias and variance. In theory, adding more features could increase the variance; but if you find this to be the case, then use regularization, which will usually eliminate the increase in variance.

  根据错误分析的见解修改输入功能：假设您的错误分析激发您创建其他功能，帮助算法消除特定类别的错误。这些新功能可能有助于偏见和差异。理论上，添加更多功能可能会增加差异;但如果你发现这种情况，那就使用正则化，这通常会消除方差的增加。

- **Modify model architecture** (such as neural network architecture) so that it is more suitable for your problem: This technique can affect both bias and variance.

  修改模型体系结构（例如神经网络体系结构），使其更适合您的问题：此技术可以影响偏差和方差。