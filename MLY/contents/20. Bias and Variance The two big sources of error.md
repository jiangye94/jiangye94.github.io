[返回目录](../MLY_index.html)

# 20. 偏见和差异：两大误差来源 Bias and Variance: The two big sources of error 

2018-10-10

[TOC]

## 学习收获

> sh

内容



Suppose your training, dev and test sets all come from the same distribution. Then you should always try to get more training data, since that can only improve performance, right? 

假设您的培训，开发和测试集都来自同一个发行版。那么你应该总是尝试获得更多的训练数据，因为这只会提高性能，对吧？

Even though having more data can’t hurt, unfortunately it doesn’t always help as much as you might hope. It could be a waste of time to work on getting more data. So, how do you decide when to add data, and when not to bother?

即使拥有更多数据也不会受到伤害，但不幸的是，它并不总是像您希望的那样有用。获取更多数据可能是浪费时间。那么，您如何决定何时添加数据，何时不打扰？ 

There are two major sources of error in machine learning: bias and variance. Understanding them will help you decide whether adding data, as well as other tactics to improve performance, are a good use of time. 

机器学习中存在两个主要的误差来源：偏差和方差。了解它们将帮助您确定添加数据以及其他提高性能的策略是否能充分利用时间。

Suppose you hope to build a cat recognizer that has 5% error. Right now, your training set has an error rate of 15%, and your dev set has an error rate of 16%. In this case, adding training data probably won’t help much. You should focus on other changes. Indeed, adding more examples to your training set only makes it harder for your algorithm to do well on the training set. (We explain why in a later chapter.) 

假设您希望构建一个误差为5％的猫识别器。现在，您的训练集的错误率为15％，您的开发设置的错误率为16％。在这种情况下，添加训练数据可能无济于事。您应该专注于其他更改。实际上，在训练集中添加更多示例只会使您的算法更难以在训练集上取得好成绩。 （我们将在后面的章节中解释原因。）

If your error rate on the training set is 15% (or 85% accuracy), but your target is 5% error (95% accuracy), then the first problem to solve is to improve your algorithm’s performance on your training set. Your dev/test set performance is usually worse than your training set performance. So if you are getting 85% accuracy on the examples your algorithm has seen, there’s no way you’re getting 95% accuracy on examples your algorithm hasn’t even seen. 

如果训练集上的错误率为15％（或准确度为85％），但目标误差为5％（准确率为95％），那么要解决的第一个问题是提高算法在训练集上的性能。您的开发/测试集性能通常比训练集性能差。因此，如果您在算法所见的示例上获得85％的准确率，那么您在算法甚至没有看到的示例上无法获得95％的准确率。

Suppose as above that your algorithm has 16% error (84% accuracy) on the dev set. We break the 16% error into two components: 

假设如上所述，您的算法在开发集上有16％的误差（84％准确度）。我们将16％的错误分解为两个组件：

- First, the algorithm’s error rate on the training set. In this example, it is 15%. We think of this informally as the algorithm’s **bias**. 

  首先，算法在训练集上的错误率。在这个例子中，它是15％。我们非正式地将此视为算法的偏见。

- Second, how much worse the algorithm does on the dev (or test) set than the training set. In this example, it does 1% worse on the dev set than the training set. We think of this informally as the algorithm’s **variance**.^6^ 

  其次，算法对开发（或测试）集的影响比训练集差多少。在这个例子中，它在开发设置上比训练集差1％。我们非正式地将此视为算法的方差

Some changes to a learning algorithm can address the first component of error—**bias**—and improve its performance on the training set. Some changes address the second component—**variance**—and help it generalize better from the training set to the dev/test sets.^7^ To select the most promising changes, it is incredibly useful to understand which of these two components of error is more pressing to address. 

对学习算法的一些更改可以解决错误偏差的第一个组成部分，并提高其在训练集上的性能。一些更改解决了第二个组件 - 方差 - 并帮助它从训练集到开发/测试集更好地概括.7要选择最有希望的更改，理解这两个错误组件中的哪一个更加迫切是非常有用的地址。

Developing good intuition about Bias and Variance will help you choose effective changes for your algorithm. 

培养关于偏差和方差的良好直觉将帮助您为算法选择有效的变化。





--------------

6 The field of statistics has more formal definitions of bias and variance that we won’t worry about. Roughly, the bias is the error rate of your algorithm on your training set when you have a very large training set. The variance is how much worse you do on the test set compared to the training set in this setting. When your error metric is mean squared error, you can write down formulas specifying these two quantities, and prove that Total Error = Bias + Variance. But for our purposes of deciding how to make progress on an ML problem, the more informal definition of bias and variance given here will suffice. 

6统计领域对偏见和差异有更正式的定义，我们不会担心。粗略地说，当您拥有非常大的训练集时，偏差是您的训练集上算法的错误率。与此设置中的训练集相比，方差是您在测试集上做的更糟糕。当您的误差度量是均方误差时，您可以记下指定这两个量的公式，并证明总误差=偏差+方差。但是，就我们决定如何在ML问题上取得进展的目的而言，这里给出的更为非正式的偏见和方差定义就足够了。

7 There are also some methods that can simultaneously reduce bias and variance, by making major changes to the system architecture. But these tend to be harder to identify and implement. 

7通过对系统架构进行重大更改，还有一些方法可以同时减少偏差和方差。但这些往往更难识别和实施。