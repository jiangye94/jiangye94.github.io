[返回目录](../MLY_index.html)

# 25. 减少可避免偏差的技术 Techniques for reducing avoidable bias

2018-10-10

[TOC]

## 学习收获

> sh

内容

 

If your learning algorithm suffers from high avoidable bias, you might try the following techniques: 

如果您的学习算法存在高度可避免的偏差，您可以尝试以下技术：

- **Increase the model size** (such as number of neurons/layers): This technique reduces bias, since it should allow you to fit the training set better. If you find that this increases variance, then use regularization, which will usually eliminate the increase in variance. 

  增加模型大小（例如神经元/层数）：这种技术可以减少偏差，因为它可以让你更好地适应训练集。如果您发现这会增加方差，则使用正则化，这通常会消除方差的增加。

- **Modify input features based on insights from error analysis**: Say your error analysis inspires you to create additional features that help the algorithm eliminate a particular category of errors. (We discuss this further in the next chapter.) These new features could help with both bias and variance. In theory, adding more features could increase the variance; but if you find this to be the case, then use regularization, which will usually eliminate the increase in variance. 

  根据错误分析的见解修改输入功能：假设您的错误分析激发您创建其他功能，帮助算法消除特定类别的错误。 （我们将在下一章进一步讨论这个问题。）这些新功能可能有助于偏见和差异。理论上，添加更多功能可能会增加差异;但如果你发现这种情况，那就使用正则化，这通常会消除方差的增加。

- **Reduce or eliminate regularization** (L2 regularization, L1 regularization, dropout): This will reduce avoidable bias, but increase variance. 

  减少或消除正则化（L2正则化，L1正则化，丢失）：这将减少可避免的偏差，但会增加方差。

- **Modify model architecture** (such as neural network architecture) so that it is more suitable for your problem: This technique can affect both bias and variance. 

  修改模型体系结构（例如神经网络体系结构），使其更适合您的问题：此技术可以影响偏差和方差。

One method that is not helpful: 

- **Add more training data**: This technique helps with variance problems, but it usually has no significant effect on bias. 

  添加更多训练数据：此技术有助于解决方差问题，但通常对偏差没有显着影响。

